{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvertTo3Channels(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvertTo3Channels, self).__init__()\n",
    "        self.conv = nn.Conv2d(512, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1, 32, 32)\n",
    "        return self.conv(x)\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.randn(1, 512, 32, 32)  # Example input tensor\n",
    "\n",
    "# Convert to 3 channels\n",
    "converter = ConvertTo3Channels()\n",
    "output_tensor = converter(input_tensor)\n",
    "print(\"Output tensor shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.7423e-01, -5.3842e-01, -8.9325e-02,  ...,  1.2988e-03,\n",
       "           -3.5259e-01, -2.5830e-01],\n",
       "          [ 2.1624e-01,  4.8532e-01,  6.4083e-01,  ..., -2.8504e-01,\n",
       "           -7.1230e-01, -1.0108e-01],\n",
       "          [-1.5990e-01,  1.6821e-01,  1.3357e-01,  ...,  5.1624e-01,\n",
       "           -9.0361e-01, -2.9006e-01],\n",
       "          ...,\n",
       "          [ 1.2752e-01, -1.5059e-02, -5.7059e-01,  ...,  3.1145e-01,\n",
       "           -5.3018e-01,  3.4071e-01],\n",
       "          [-2.6404e-01, -2.0116e-01,  4.6474e-01,  ...,  5.6449e-01,\n",
       "           -4.7560e-01, -4.2593e-01],\n",
       "          [ 3.4443e-01, -5.0213e-01,  5.0383e-01,  ..., -7.3862e-01,\n",
       "           -6.0495e-01, -8.9527e-01]],\n",
       "\n",
       "         [[-8.2730e-02, -2.3951e-01, -1.1211e+00,  ...,  1.3933e-01,\n",
       "            2.3821e-02,  7.3373e-01],\n",
       "          [-9.5581e-01,  1.1010e-01,  3.0707e-01,  ...,  5.1498e-01,\n",
       "            4.7180e-01, -7.0015e-02],\n",
       "          [ 3.9114e-01, -6.1494e-01,  9.4509e-01,  ...,  4.6940e-01,\n",
       "           -1.2980e-01,  1.9701e-01],\n",
       "          ...,\n",
       "          [ 4.6562e-01, -8.2189e-02, -4.5617e-01,  ...,  2.9411e-01,\n",
       "            1.6981e-01, -7.6772e-01],\n",
       "          [-2.6912e-01,  5.1781e-01,  2.3857e-01,  ..., -4.0779e-01,\n",
       "           -1.1668e+00,  2.0073e-01],\n",
       "          [ 2.6691e-01, -6.8253e-01,  1.6479e-01,  ...,  4.1205e-01,\n",
       "           -2.1915e-01,  2.0170e-01]],\n",
       "\n",
       "         [[-5.3817e-01,  6.2291e-01, -7.7696e-01,  ...,  6.0189e-01,\n",
       "           -7.8758e-02,  1.0907e+00],\n",
       "          [-2.9386e-01, -3.8636e-02, -2.5476e-01,  ...,  3.5138e-01,\n",
       "            1.6364e+00,  4.3146e-02],\n",
       "          [-6.4337e-01, -5.0202e-01,  1.4120e-01,  ...,  4.0839e-01,\n",
       "            1.0618e-01,  5.7824e-01],\n",
       "          ...,\n",
       "          [ 7.6600e-03, -2.8817e-01, -1.0266e-01,  ...,  7.2536e-02,\n",
       "            3.8322e-01,  7.2242e-01],\n",
       "          [ 3.2039e-01, -1.4456e+00,  1.3708e-01,  ...,  3.7495e-01,\n",
       "           -1.8122e-01,  1.9021e+00],\n",
       "          [ 3.2759e-01,  1.4649e-01,  1.2224e+00,  ...,  5.0172e-01,\n",
       "            7.9365e-01,  7.9666e-01]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class ChannelSpatialAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(ChannelSpatialAttention, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_out = self.channel_attention(x)\n",
    "        spatial_out = self.spatial_attention(x)\n",
    "        out = torch.mul(channel_out, spatial_out)\n",
    "        return out\n",
    "\n",
    "class CombinedAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=16, kernel_size=7):\n",
    "        super(CombinedAttention, self).__init__()\n",
    "        self.attention = ChannelSpatialAttention(in_channels, ratio, kernel_size)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.conv_final = nn.Conv2d(in_channels + in_channels, in_channels, kernel_size=1)  # Updated to include input channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x)\n",
    "        conv_out = self.conv(att)\n",
    "        tanh_out = self.tanh(conv_out)\n",
    "        combined = torch.cat([tanh_out, x], dim=1)  # Concatenate along channel dimension\n",
    "        final_out = self.conv_final(combined)  # Apply conv to match input size\n",
    "        return final_out\n",
    "\n",
    "# Example usage:\n",
    "feature = torch.randn(8, 128 , 128 , 128)  # Input feature\n",
    "model = CombinedAttention(in_channels=128)\n",
    "output = model(feature)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg torch.Size([8, 1, 256, 256])\n",
      "max torch.Size([8, 1, 256, 256])\n",
      "con torch.Size([8, 2, 256, 256])\n",
      "conv tensor([[[[-5.4023e-01, -8.9021e-01, -6.2424e-01,  ..., -4.7003e-01,\n",
      "           -2.9704e-01, -2.3351e-01],\n",
      "          [-8.3550e-01, -8.9075e-01, -1.4888e-01,  ..., -2.1433e-01,\n",
      "            4.5876e-01,  6.7162e-01],\n",
      "          [-1.5441e-01, -8.4979e-02,  7.6736e-01,  ...,  9.7400e-01,\n",
      "            1.3031e+00,  9.8229e-01],\n",
      "          ...,\n",
      "          [-1.3581e-01, -1.9889e-01,  8.7171e-01,  ...,  1.3067e+00,\n",
      "            1.9177e+00,  1.6084e+00],\n",
      "          [ 1.6394e-02,  1.1206e-01,  8.9854e-01,  ...,  1.4135e+00,\n",
      "            1.7978e+00,  1.7870e+00],\n",
      "          [ 4.1042e-01,  4.9643e-01,  1.0271e+00,  ...,  1.6706e+00,\n",
      "            1.6909e+00,  1.6232e+00]]],\n",
      "\n",
      "\n",
      "        [[[-7.2294e-01, -8.4463e-01, -4.2732e-01,  ..., -5.5632e-01,\n",
      "           -2.9744e-01, -9.8651e-02],\n",
      "          [-7.8191e-01, -8.7735e-01, -3.6199e-01,  ..., -1.0555e-02,\n",
      "            4.7615e-01,  4.6984e-01],\n",
      "          [-1.2327e-01, -4.5316e-02,  5.5136e-01,  ...,  4.5236e-01,\n",
      "            1.1571e+00,  1.1152e+00],\n",
      "          ...,\n",
      "          [-1.5423e-01, -3.6930e-01,  7.0298e-01,  ...,  1.3201e+00,\n",
      "            1.6221e+00,  1.7419e+00],\n",
      "          [ 2.2953e-01,  1.6459e-01,  6.5341e-01,  ...,  1.6971e+00,\n",
      "            2.0631e+00,  1.8482e+00],\n",
      "          [ 4.2405e-01,  6.7756e-01,  9.1810e-01,  ...,  1.5828e+00,\n",
      "            1.9115e+00,  1.7701e+00]]],\n",
      "\n",
      "\n",
      "        [[[-7.5124e-01, -9.3162e-01, -3.0555e-01,  ..., -4.0787e-01,\n",
      "           -4.1179e-01, -1.2435e-01],\n",
      "          [-6.1235e-01, -7.4941e-01,  2.3843e-03,  ...,  1.3344e-01,\n",
      "            4.2381e-01,  5.2045e-01],\n",
      "          [-1.4394e-02,  9.5135e-02,  4.4722e-01,  ...,  8.0083e-01,\n",
      "            1.2638e+00,  1.0607e+00],\n",
      "          ...,\n",
      "          [-2.0642e-01, -2.6611e-02,  7.2916e-01,  ...,  1.5725e+00,\n",
      "            1.6528e+00,  1.5777e+00],\n",
      "          [ 2.4863e-01,  9.9102e-02,  9.1875e-01,  ...,  1.4859e+00,\n",
      "            2.1040e+00,  1.8859e+00],\n",
      "          [ 2.9050e-01,  5.7021e-01,  1.0187e+00,  ...,  1.9870e+00,\n",
      "            2.1252e+00,  1.5847e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.2391e-01, -8.6930e-01, -4.9185e-01,  ..., -3.0414e-01,\n",
      "           -4.8418e-01, -8.2356e-02],\n",
      "          [-7.9887e-01, -8.1727e-01, -6.4707e-04,  ..., -7.5792e-02,\n",
      "            4.9759e-01,  6.4857e-01],\n",
      "          [ 4.9354e-02,  2.7016e-01,  5.6947e-01,  ...,  8.6502e-01,\n",
      "            1.0849e+00,  1.1035e+00],\n",
      "          ...,\n",
      "          [ 7.0741e-02, -6.4599e-02,  7.8741e-01,  ...,  1.5177e+00,\n",
      "            1.8439e+00,  1.7535e+00],\n",
      "          [ 1.0156e-01,  2.5491e-01,  8.6071e-01,  ...,  1.3360e+00,\n",
      "            1.8123e+00,  2.1473e+00],\n",
      "          [ 2.8045e-01,  4.9073e-01,  1.1122e+00,  ...,  1.8931e+00,\n",
      "            1.7759e+00,  1.6520e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.1271e-01, -8.7086e-01, -4.0950e-01,  ..., -6.2677e-01,\n",
      "           -3.5062e-01, -1.0081e-01],\n",
      "          [-7.6039e-01, -6.4407e-01, -2.6551e-01,  ..., -2.2450e-01,\n",
      "            5.7459e-01,  3.9244e-01],\n",
      "          [-2.4058e-02, -1.5482e-02,  6.1850e-01,  ...,  7.6325e-01,\n",
      "            1.2219e+00,  1.0760e+00],\n",
      "          ...,\n",
      "          [-7.6923e-02, -1.8897e-01,  7.0691e-01,  ...,  1.2253e+00,\n",
      "            1.7456e+00,  1.9188e+00],\n",
      "          [ 1.2311e-02, -4.3512e-02,  9.2985e-01,  ...,  1.6656e+00,\n",
      "            1.9323e+00,  1.8502e+00],\n",
      "          [ 2.1832e-01,  5.7456e-01,  1.1968e+00,  ...,  1.8752e+00,\n",
      "            1.8803e+00,  1.7035e+00]]],\n",
      "\n",
      "\n",
      "        [[[-6.6800e-01, -8.7575e-01, -3.6873e-01,  ..., -6.4016e-01,\n",
      "           -4.4967e-01, -1.9965e-01],\n",
      "          [-6.5537e-01, -8.2557e-01, -1.8708e-01,  ..., -2.1479e-01,\n",
      "            4.3701e-01,  5.1276e-01],\n",
      "          [ 3.2734e-02, -6.5537e-03,  3.3577e-01,  ...,  6.2218e-01,\n",
      "            1.3114e+00,  1.1782e+00],\n",
      "          ...,\n",
      "          [-1.8830e-01, -1.5272e-01,  6.1084e-01,  ...,  1.1752e+00,\n",
      "            1.7739e+00,  1.7595e+00],\n",
      "          [-2.4464e-05, -9.4198e-04,  6.8630e-01,  ...,  1.6745e+00,\n",
      "            1.7993e+00,  1.9560e+00],\n",
      "          [ 1.9377e-01,  4.7687e-01,  1.0412e+00,  ...,  1.7100e+00,\n",
      "            1.7604e+00,  1.6451e+00]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Output shape: torch.Size([8, 256, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)  # Updated convolution layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        print(\"Avg\" , x_avg.shape)\n",
    "        x_max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        print(\"max\" , x_max.shape)\n",
    "        x_concat = torch.cat([x_avg, x_max], dim=1)\n",
    "        print(\"con\" , x_concat.shape)\n",
    "        print(\"conv\" , self.conv(x_concat))\n",
    "        return self.sigmoid(self.conv(x_concat))\n",
    "\n",
    "class ChannelSpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=16, kernel_size=7):\n",
    "        super(ChannelSpatialAttention, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_attention = SpatialAttention(in_channels * 2, kernel_size)  # Updated input channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_out = self.channel_attention(x)\n",
    "        spatial_out = self.spatial_attention(x)\n",
    "        out = torch.mul(channel_out, spatial_out)\n",
    "        return out\n",
    "\n",
    "class CombinedAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=16, kernel_size=7):\n",
    "        super(CombinedAttention, self).__init__()\n",
    "        self.attention = ChannelSpatialAttention(in_channels, ratio, kernel_size)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.conv_final = nn.Conv2d(in_channels * 2, in_channels, kernel_size=1)  # Updated to include input channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        att = self.attention(x)\n",
    "        conv_out = self.conv(att)\n",
    "        tanh_out = self.tanh(conv_out)\n",
    "        combined = torch.cat([tanh_out, x], dim=1)  # Concatenate along channel dimension\n",
    "        final_out = self.conv_final(combined)  # Apply conv to match input size\n",
    "        return final_out\n",
    "\n",
    "# Example usage:\n",
    "feature = torch.randn(8, 256 , 256 , 256)  # Input feature\n",
    "in_channels = feature.size(1)  # Get the number of input channels dynamically\n",
    "model = CombinedAttention(in_channels=in_channels)\n",
    "output = model(feature)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
